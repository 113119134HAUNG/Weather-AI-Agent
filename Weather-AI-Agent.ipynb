{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **è­¦å‘Šï¼šè©²è³‡æ–™åº«å’Œå·¥å…·çš†ç‚ºæœ¬äººæ‰€è£½ï¼Œè«‹å‹¿ä»»æ„ä¸‹è¼‰ã€‚**F113119134 é«˜é›„ç§‘æŠ€å¤§å­¸"
      ],
      "metadata": {
        "id": "EjPjUrgTO9GB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piKeY0lFNfIf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4713f4d-2c3c-49a6-f3ed-89b388c0cd74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "å…¨éƒ¨å®‰è£å®Œç•¢!\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/113119134HAUNG/Weather-AI-Agent.git> /dev/null 2>&1 && \\\n",
        "!!bash install.sh > /dev/null 2>&1 && \\\n",
        "echo \"å…¨éƒ¨å®‰è£å®Œç•¢!\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLoXTRx6OhnE"
      },
      "source": [
        "# **1. è®€å…¥å¥—ä»¶ã€è‡ªè£½å·¥å…·å’Œè³‡æ–™é›†**\n",
        "\n",
        "**é€™è£¡ä¸»è¦ç”¨ `OpenHowNet` åŒ¯å…¥èªç¾©(è©)è³‡æ–™åº«,opencc-å› ç‚ºè³‡æ–™åº«æ˜¯ç°¡å­—,æ‰€ä»¥éœ€è¦ç¹è½‰ç°¡ï¼Œè¼¸å‡ºå¯ä»¥å†ç°¡è½‰ç¹ã€‚**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XUd8vStehUd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ddbf3e9-d75d-4598-d1d3-cf33b013a171"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vector_utils_advanced æ¨¡çµ„è¼‰å…¥å®Œæˆï¼\n"
          ]
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import re\n",
        "import os\n",
        "import json\n",
        "import faiss\n",
        "import jieba\n",
        "import torch\n",
        "import openai\n",
        "import requests\n",
        "import importlib\n",
        "import OpenHowNet\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gradio as gr\n",
        "import sememe_tools as st\n",
        "import matplotlib.pyplot as pltm\n",
        "import vector_utils_advanced as vu\n",
        "importlib.reload(vu)\n",
        "\n",
        "from tqdm import tqdm\n",
        "from pprint import pprint\n",
        "from opencc import OpenCC\n",
        "from datetime import datetime\n",
        "from google.colab import userdata\n",
        "from OpenHowNet import HowNetDict\n",
        "from datasets import load_dataset\n",
        "from collections import defaultdict\n",
        "from multi_turn_qa import multi_turn_qa\n",
        "from sklearn.preprocessing import normalize\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from anytree import Node, RenderTree, AsciiStyle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_X1eul7llVcf",
        "outputId": "fac445e0-c47e-454b-ef16-35c1315bfd50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'q': 'æŸ³æ±Ÿçš„ä¸»è¦æ”¯æµçš„å‘æºåœ°æ˜¯å“ªé‡Œï¼Ÿ', 'path': [['æŸ³æ±Ÿ ||| 2970218', 'ä¸»è¦æ”¯æµ', 'å¯¨è’¿æ²³ ||| 7476446'], ['å¯¨è’¿æ²³ ||| 7476446', 'å‘æºäº', 'è´µå·çœé»å¹³å¿é«˜æ´‹ä¹¡ ||| 0']]}\n"
          ]
        }
      ],
      "source": [
        "# è®€å– NLPCC-MH è³‡æ–™é›†\n",
        "with open(\"/content/NLPCC-MH/data/nlpcc-mh.train.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "print(data[0])  # ç¢ºèªç¬¬ä¸€ç­†å…§å®¹"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9YAU956yWT2"
      },
      "outputs": [],
      "source": [
        "# è®€å–è‡ªè£½ Synonyms è³‡æ–™\n",
        "with open(\"/content/sememe_synonym_OK.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    custom_synonyms = json.load(f)\n",
        "\n",
        "# å»ºç«‹ custom_synonym_map, location_terms, weather_terms\n",
        "custom_synonym_map = {}\n",
        "location_terms = set()\n",
        "weather_terms = set()\n",
        "\n",
        "for key, entry in custom_synonyms.items():\n",
        "    zh_entry = entry.get(\"zh\", key)\n",
        "    zh_words = zh_entry if isinstance(zh_entry, list) else [zh_entry]\n",
        "\n",
        "    # é¸ç¬¬ä¸€å€‹ä½œç‚ºæ¨™æº–åŒ–è©\n",
        "    standard_word = st.normalize_text(zh_words[0])\n",
        "\n",
        "    # ä¸»è© + åŒç¾©è©éƒ½æ¨™æº–åŒ–\n",
        "    for word in zh_words:\n",
        "        if isinstance(word, str):\n",
        "            custom_synonym_map[st.normalize_text(word)] = standard_word\n",
        "    for syn in entry.get(\"synonyms\", []):\n",
        "        if isinstance(syn, str):\n",
        "            custom_synonym_map[st.normalize_text(syn)] = standard_word\n",
        "\n",
        "    # åˆ†é¡åœ°åèˆ‡å¤©æ°£è©\n",
        "    categories = entry.get(\"categories\", {})\n",
        "    target_set = location_terms if any(c in categories for c in [\"ç›´è½„å¸‚\", \"çœè½„å¸‚\", \"ç¸£\", \"ç¸£è½„å¸‚\", \"å€\", \"é®\", \"é„‰\"]) else weather_terms\n",
        "    for w in zh_words:\n",
        "        if isinstance(w, str):\n",
        "            target_set.add(st.normalize_text(w))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "# æ‰å¹³åŒ–è³‡æ–™çµæ§‹\n",
        "def flatten_sememe_data(data, path=None, results=None):\n",
        "    if results is None:\n",
        "        results = {}\n",
        "    if path is None:\n",
        "        path = []\n",
        "    if isinstance(data, dict):\n",
        "        if \"items\" in data:\n",
        "            for item in data[\"items\"]:\n",
        "                key = item.get(\"id\") or item.get(\"zh\") or item.get(\"en\")\n",
        "                if not key:\n",
        "                    continue\n",
        "                linked = item.get(\"linked_sememe\", {})\n",
        "                synonyms = []\n",
        "                if isinstance(linked, dict):\n",
        "                    zh_syns = linked.get(\"zh\", [])\n",
        "                    en_syns = linked.get(\"en\", [])\n",
        "                    zh_syns = zh_syns if isinstance(zh_syns, list) else [zh_syns]\n",
        "                    en_syns = en_syns if isinstance(en_syns, list) else [en_syns]\n",
        "                    synonyms = list(set(filter(None, zh_syns + en_syns)))\n",
        "                results[key] = {\n",
        "                    \"zh\": item.get(\"zh\", \"\"),\n",
        "                    \"en\": item.get(\"en\", \"\"),\n",
        "                    \"synonyms\": synonyms,\n",
        "                    \"categories\": path.copy()\n",
        "                }\n",
        "        if \"categories\" in data:\n",
        "            for cat_name, cat_data in data[\"categories\"].items():\n",
        "                flatten_sememe_data(cat_data, path + [cat_name], results)\n",
        "    return results\n",
        "\n",
        "# ç°¡æ˜“æ–‡å­—æ­£è¦åŒ–\n",
        "class SimpleNormalizer:\n",
        "    @staticmethod\n",
        "    def normalize_text(text):\n",
        "        return re.sub(r\"\\s+\", \"\", text.lower())\n",
        "\n",
        "st = SimpleNormalizer()\n",
        "\n",
        "# é¡åˆ¥å®šç¾©èˆ‡å„ªå…ˆé †åº\n",
        "CATEGORY_TREE = {\n",
        "    \"geo_feature\": [\"åœ°å½¢åœ°è²Œ\", \"æ°´æ–‡ç’°å¢ƒ\", \"ç«å±±èˆ‡åœ°è³ª\", \"æµ·æ´‹èˆ‡æ²¿å²¸\", \"åœ‹å®¶å…¬åœ’èˆ‡è‡ªç„¶ä¿è­·å€\", \"ç‰¹æ®Šè‡ªç„¶æ™¯è§€\"],\n",
        "    \"climate\": [\"æ°£å€™\", \"æ°£å€™è®Šé·\", \"å­£é¢¨\", \"ä¹¾å­£\", \"æ¿•å­£\", \"æ°£å€™ç½å®³\"],\n",
        "    \"weather\": [\"å¤©æ°£\", \"æ™´æœ—èˆ‡é›²é‡è®ŠåŒ–\", \"é™æ°´èˆ‡é›·é›¨ç¾è±¡\", \"ç‰¹æ®Šé™æ°´èˆ‡å†°é›ªç¾è±¡\", \"èƒ½è¦‹åº¦èˆ‡ç©ºæ°£ç¾è±¡\", \"æ¥µç«¯å¤©æ°£èˆ‡ç½å®³\", \"é‹’é¢èˆ‡æ°£å€™è®ŠåŒ–\", \"å¤©æ°£ç¾è±¡\"],\n",
        "    \"location\": [\"ç›´è½„å¸‚\", \"çœè½„å¸‚\", \"ç¸£\", \"ç¸£è½„å¸‚\", \"å€\", \"é„‰\", \"é®\", \"åŸå¸‚\", \"éƒ½å¸‚å€\", \"æ‘\", \"é‡Œ\", \"è¡Œæ”¿å€\"]\n",
        "}\n",
        "CATEGORY_PRIORITY = [\"geo_feature\", \"climate\", \"weather\", \"location\"]\n",
        "\n",
        "# èªæ„çŸ¯æ­£è¨­å®š\n",
        "WEATHER_OVERRIDE = [\"å†·é‹’\", \"æš–é‹’\", \"æ»¯ç•™é‹’\", \"é‹’é¢é›¨\", \"é›·é™£é›¨\", \"çŸ­æ™‚å¼·é™é›¨\", \"é–“æ­‡æ€§å°é›¨\", \"éœœå‡\", \"æšæ²™\", \"æ™´æœ—ç„¡é›²\"]\n",
        "CLIMATE_EXCLUDE_FROM_WEATHER = [\"å¼·é™é›¨äº‹ä»¶\", \"å¹´é™é›¨é‡\", \"æ¢…é›¨å­£\"]\n",
        "\n",
        "# ç²¾æº–åˆ†é¡ + èªæ„è£œæ­£ + fallback åŸå¸‚åˆ¤æ–·\n",
        "def build_precise_maps(flattened_data):\n",
        "    category_term_sets = {cat: set() for cat in CATEGORY_TREE.keys()}\n",
        "    custom_synonym_map = {}\n",
        "    classified_terms = set()\n",
        "    unclassified_terms = set()\n",
        "    reclassified_terms = []\n",
        "\n",
        "    for key, entry in flattened_data.items():\n",
        "        zh_entry = entry.get(\"zh\", key)\n",
        "        synonyms = entry.get(\"synonyms\", [])\n",
        "        zh_words = zh_entry if isinstance(zh_entry, list) else [zh_entry]\n",
        "        standard_word = st.normalize_text(zh_words[0]) if zh_words else None\n",
        "        if not standard_word:\n",
        "            continue\n",
        "\n",
        "        for word in zh_words + synonyms:\n",
        "            if isinstance(word, str):\n",
        "                custom_synonym_map[st.normalize_text(word)] = standard_word\n",
        "\n",
        "        path = entry.get(\"categories\", [])\n",
        "        entry[\"classification\"] = []\n",
        "        entry[\"triggered_by\"] = []\n",
        "\n",
        "        classified = False\n",
        "        for cat in CATEGORY_PRIORITY:\n",
        "            keywords = CATEGORY_TREE[cat]\n",
        "            matched = [p for p in path if any(k in p for k in keywords)]\n",
        "            if matched:\n",
        "                entry[\"classification\"].append(cat)\n",
        "                entry[\"triggered_by\"].extend(matched)\n",
        "                category_term_sets[cat].add(standard_word)\n",
        "                classified_terms.add(standard_word)\n",
        "                classified = True\n",
        "                break\n",
        "\n",
        "        # fallback åŸå¸‚è©å°¾åˆ¤æ–·\n",
        "        if not classified:\n",
        "            location_suffixes = [\"å¸‚\", \"å€\", \"é„‰\", \"é®\", \"æ‘\", \"é‡Œ\", \"å³¶\"]\n",
        "            if any(isinstance(w, str) and w.endswith(tuple(location_suffixes)) for w in zh_words):\n",
        "                category_term_sets[\"location\"].add(standard_word)\n",
        "                entry[\"classification\"].append(\"location\")\n",
        "                entry[\"triggered_by\"].append(\"suffix_match\")\n",
        "                classified_terms.add(standard_word)\n",
        "                classified = True\n",
        "\n",
        "        if not classified:\n",
        "            unclassified_terms.add(standard_word)\n",
        "\n",
        "    # èªæ„çŸ¯æ­£ weather èˆ‡ climate çš„èª¤æ­¸é¡\n",
        "    for word in list(classified_terms):\n",
        "        original = None\n",
        "        for cat, terms in category_term_sets.items():\n",
        "            if word in terms:\n",
        "                original = cat\n",
        "                break\n",
        "\n",
        "        if any(keyword in word for keyword in WEATHER_OVERRIDE):\n",
        "            if word not in CLIMATE_EXCLUDE_FROM_WEATHER and original != \"weather\":\n",
        "                if original:\n",
        "                    category_term_sets[original].remove(word)\n",
        "                category_term_sets[\"weather\"].add(word)\n",
        "                reclassified_terms.append((word, original, \"weather\"))\n",
        "\n",
        "    return custom_synonym_map, category_term_sets, classified_terms, unclassified_terms, reclassified_terms\n",
        "\n",
        "# ä¸»ç¨‹å¼\n",
        "with open(\"/content/sememe_synonym_OK.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_data = json.load(f)\n",
        "\n",
        "taiwan_data = raw_data.get(\"Country\", {}).get(\"categories\", {}).get(\"Taiwan\", {})\n",
        "flattened_data = flatten_sememe_data(taiwan_data)\n",
        "\n",
        "custom_synonym_map, category_term_sets, classified_terms, unclassified_terms, reclassified_terms = build_precise_maps(flattened_data)\n",
        "\n",
        "# çµæœè¼¸å‡º\n",
        "print(f\"\\nè‡ªè¨‚ Synonym Map å·²è¼‰å…¥ï¼Œå…± {len(custom_synonym_map)} ç­†\\n\")\n",
        "for cat, terms in category_term_sets.items():\n",
        "    print(f\"åˆ†é¡ã€Œ{cat}ã€è©å½™æ•¸é‡ï¼š{len(terms)}\")\n",
        "    print(f\"ç¯„ä¾‹ï¼š{list(terms)[:10]}\\n\")\n",
        "\n",
        "total_classified = sum(len(terms) for terms in category_term_sets.values())\n",
        "print(f\"å·²åˆ†é¡è©å½™ç¸½æ•¸ï¼š{total_classified}\")\n",
        "print(f\"æœªåˆ†é¡è©å½™ç¸½æ•¸ï¼š{len(unclassified_terms)}\")\n",
        "if unclassified_terms:\n",
        "    print(f\"æœªåˆ†é¡ç¯„ä¾‹ï¼š{list(unclassified_terms)[:10]}\")\n",
        "if reclassified_terms:\n",
        "    print(\"\\nèªæ„çŸ¯æ­£é‡æ–°åˆ†é¡ï¼š\")\n",
        "    for word, from_cat, to_cat in reclassified_terms:\n",
        "        print(f\"    {word}ï¼š{from_cat} â†’ {to_cat}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac7Dzr6JOQjW",
        "outputId": "4b7ccf54-0239-43c8-87ef-c42af690cb2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… è‡ªè¨‚ Synonym Map å·²è¼‰å…¥ï¼Œå…± 919 ç­†\n",
            "\n",
            "âœ… åˆ†é¡ã€Œgeo_featureã€è©å½™æ•¸é‡ï¼š49\n",
            "ç¯„ä¾‹ï¼š['å¢¾ä¸åœ‹å®¶å…¬åœ’', 'ä¸­å¤®å±±è„ˆ', 'èŠ±è“®æºª', 'åŒ—æµ·å²¸', 'å±æ±å¹³åŸ', 'åˆæ­¡å±±', 'æ›¾æ–‡æ°´åº«', 'å—æµ·å²¸', 'å¤§é›ªå±±', 'å¢¾ä¸çŠç‘šç¤']\n",
            "\n",
            "âœ… åˆ†é¡ã€Œclimateã€è©å½™æ•¸é‡ï¼š30\n",
            "ç¯„ä¾‹ï¼š['å±±å€æ°£å€™å·®ç•°', 'æ°£å€™ç½å®³æ‡‰å°', 'ä¹¾å­£', 'å—éƒ¨ä¹¾æ—±å€', 'æ±åŒ—å­£é¢¨å½±éŸ¿', 'ç†±å¸¶æ°£å€™', 'æµ·å²¸æ°£å€™ç‰¹å¾µ', 'äºç†±å¸¶æ°£å€™', 'å¹´é™é›¨é‡', 'å¾®æ°£å€™åˆ†å¸ƒ']\n",
            "\n",
            "âœ… åˆ†é¡ã€Œweatherã€è©å½™æ•¸é‡ï¼š40\n",
            "ç¯„ä¾‹ï¼š['é–“æ­‡æ€§å°é›¨', 'å†·é‹’é€šé', 'çŸ­æ™‚å¼·é™é›¨', 'å¼·é›·é™£é›¨', 'å¤§é›¨', 'æšæ²™', 'éœœå‡', 'æ™´æœ—ç„¡é›²', 'æš–é‹’é€šé', 'å±€éƒ¨é©Ÿé›¨']\n",
            "\n",
            "âœ… åˆ†é¡ã€Œlocationã€è©å½™æ•¸é‡ï¼š322\n",
            "ç¯„ä¾‹ï¼š['è¥¿æ¸¯å€', 'å½°åŒ–å¸‚', 'ç¾©ç«¹é„‰', 'å…‰å¾©é„‰', 'å¤§åŸ”é„‰', 'è¿æ¡é„‰', 'è‡ºè¥¿é„‰', 'ç¶ å³¶é„‰', 'è¥¿æ¹–é„‰', 'æ¨¹æ—å€']\n",
            "\n",
            "âœ… å·²åˆ†é¡è©å½™ç¸½æ•¸ï¼š441\n",
            "âœ… æœªåˆ†é¡è©å½™ç¸½æ•¸ï¼š0\n",
            "\n",
            "âœ… èªæ„çŸ¯æ­£é‡æ–°åˆ†é¡ï¼š\n",
            "    æš–é‹’é€šéï¼šclimate â†’ weather\n",
            "    é‹’é¢é›¨ï¼šclimate â†’ weather\n",
            "    å†·é‹’é€šéï¼šclimate â†’ weather\n",
            "    æ»¯ç•™é‹’ï¼šclimate â†’ weather\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# è¨­å®š custom_synonym_map å’Œ custom_synonyms çµ¦ sememe_tools\n",
        "st.set_custom_synonym_map(custom_synonym_map)\n",
        "st.set_custom_synonyms(custom_synonyms)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vec9xmigHbVp",
        "outputId": "bbdfbb9d-bedb-4dd4-8482-56229c2f816d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "è‡ªè¨‚ Synonym Map å·²è¼‰å…¥ï¼Œå…± 3 ç­†\n",
            "è‡ªè¨‚ Synonyms æ“´å±•è³‡æ–™å·²è¼‰å…¥ï¼Œå…± 1 ç­†\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MspOblW1I6J"
      },
      "outputs": [],
      "source": [
        "# è™•ç† NLPCC-MH å–®ç­†å•ç­”æ¨£æœ¬\n",
        "def process_nlpccmh_sample(sample, base_id=\"nlpcc\", index=0):\n",
        "    result = []\n",
        "    question = sample[\"q\"]\n",
        "\n",
        "    sememe_analysis = st.analyze_sentence(question)\n",
        "    sememe_tags = sememe_analysis[\"sememe_tags\"]\n",
        "    sememe_map = sememe_analysis[\"sememe_map\"]\n",
        "\n",
        "    for path_id, triple in enumerate(sample.get(\"path\", [])):\n",
        "        head = triple[0].split(\" ||| \")[0]\n",
        "        relation = triple[1]\n",
        "        tail = triple[2].split(\" ||| \")[0]\n",
        "        sentence = f\"{head} {relation} {tail}\"\n",
        "\n",
        "        result.append({\n",
        "            \"id\": f\"{base_id}_{index}_{path_id}\",\n",
        "            \"question\": question,\n",
        "            \"question_sememe\": sememe_tags,\n",
        "            \"question_sememe_map\": sememe_map,\n",
        "            \"triple_sentence\": sentence,\n",
        "            \"head\": head,\n",
        "            \"relation\": relation,\n",
        "            \"tail\": tail\n",
        "        })\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y44Gexv60-mR"
      },
      "outputs": [],
      "source": [
        "# åˆ†æ‰¹è™•ç†æ•´ä»½è³‡æ–™é›†ä¸¦è¼¸å‡ºç‚º JSONL\n",
        "def process_nlpccmh_file(input_path, output_path, batch_size=1000):\n",
        "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as out_f:\n",
        "        for i in range(0, len(data), batch_size):\n",
        "            batch = data[i:i + batch_size]\n",
        "            for j, sample in enumerate(tqdm(batch, desc=f\"Batch {i // batch_size}\")):\n",
        "                processed = process_nlpccmh_sample(sample, index=i + j)\n",
        "                for entry in processed:\n",
        "                    out_f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwvfKNEllmkf"
      },
      "outputs": [],
      "source": [
        "# å•Ÿå‹•è™•ç†æµç¨‹\n",
        "process_nlpccmh_file(\n",
        "    input_path=\"/content/NLPCC-MH/data/nlpcc-mh.train.json\",\n",
        "    output_path=\"/content/NLPCC-MH/data/nlpcc-mh.train_sememe.jsonl\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3H4OR5ApdiG"
      },
      "source": [
        "**æ¸¬è©¦**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68wCnYISZZZy"
      },
      "outputs": [],
      "source": [
        "# æ¸¬è©¦è³‡æ–™\n",
        "sememe_map = {\n",
        "    \"ç™¼æºåœ°\": [\"place|åœ°æ–¹\", \"ComeToWorld|å•ä¸–\"],\n",
        "    \"æŸ³æ±Ÿ\": [\"waters|æ°´åŸŸ\", \"China|ä¸­åœ‹\"]\n",
        "}\n",
        "question = \"æŸ³æ±Ÿçš„ä¸»è¦æ”¯æµçš„å‘æºåœ°æ˜¯å“ªé‡Œï¼Ÿ\"\n",
        "\n",
        "# å¾ sememe_tool.py èª¿ç”¨ generate_augmented_query\n",
        "augmented_query = st.generate_augmented_query(question, sememe_map)\n",
        "\n",
        "# é¡¯ç¤ºçµæœ\n",
        "print(augmented_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrBsCaGR44mF"
      },
      "source": [
        "**åŸ·è¡ŒåµŒå…¥å’Œå»ºç«‹å‘é‡åº«**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1w8Lz0of8XuU"
      },
      "outputs": [],
      "source": [
        "# è¨­å®šèˆ‡æ¨¡å‹åˆå§‹åŒ–\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-base-zh\")\n",
        "hf_model = AutoModel.from_pretrained(\"BAAI/bge-base-zh\").to(device).eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVy11SkjGIFt"
      },
      "outputs": [],
      "source": [
        "#æ§‹å»º NLPCC-MH æ“´å……å¾Œçš„å‘é‡åº«\n",
        "def prepare_nlpccmh_augmented_data(\n",
        "    input_path, index_path, meta_path, model, tokenizer, device, pooling=\"cls\"\n",
        "):\n",
        "    assert os.path.exists(input_path), f\"æ‰¾ä¸åˆ°è¼¸å…¥æª”æ¡ˆ: {input_path}\"\n",
        "\n",
        "    texts, ids, metas = [], [], []\n",
        "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for i, line in enumerate(tqdm(f, desc=\"è™•ç† NLPCC-MH è³‡æ–™\")):\n",
        "            entry = json.loads(line.strip())\n",
        "            question = entry[\"question\"]\n",
        "            sememe_map = entry[\"question_sememe_map\"]\n",
        "\n",
        "            pseudo_text = \"ï¼›\".join(st.format_sememe_map(sememe_map, style=\"display\"))\n",
        "            merged_query = st.generate_augmented_query(question, sememe_map)\n",
        "\n",
        "            texts.append(merged_query)\n",
        "            ids.append(entry[\"id\"])\n",
        "            metas.append({\n",
        "                \"query\": question,\n",
        "                \"sememe\": pseudo_text\n",
        "            })\n",
        "\n",
        "    vu.build_faiss_index_and_save(\n",
        "        texts=texts,\n",
        "        ids=ids,\n",
        "        meta_list=metas,\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        device=device,\n",
        "        index_path=index_path,\n",
        "        meta_path=meta_path,\n",
        "        pooling=pooling\n",
        "    )\n",
        "\n",
        "    print(f\"NLPCC-MHå‘é‡åº«å·²å»ºç½®å®Œæˆï¼Œå…± {len(texts)} ç­†è³‡æ–™ã€‚\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8by82RzmJlh9"
      },
      "outputs": [],
      "source": [
        "# è™•ç†è‡ªè£½åŒç¾©è©è³‡æ–™ï¼Œå»ºç«‹å‘é‡åº«\n",
        "def prepare_custom_augmented_data(synonym_path, index_path, meta_path, model, tokenizer, device, pooling=\"cls\"):\n",
        "    assert os.path.exists(synonym_path), f\"æ‰¾ä¸åˆ°è¼¸å…¥æª”æ¡ˆ: {synonym_path}\"\n",
        "\n",
        "    texts, ids, metas = [], [], []\n",
        "    with open(synonym_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        synonym_data = json.load(f)\n",
        "\n",
        "    for i, (key, entry) in enumerate(tqdm(synonym_data.items(), desc=\"è™•ç†è‡ªè£½åŒç¾©è©è³‡æ–™\")):\n",
        "        zh_entry = entry.get(\"zh\", key)\n",
        "        synonyms = entry.get(\"synonyms\", [])\n",
        "        categories = entry.get(\"categories\", {})\n",
        "\n",
        "        if isinstance(zh_entry, list):\n",
        "            standard_word = zh_entry[0]\n",
        "            parts = zh_entry.copy()\n",
        "        else:\n",
        "            standard_word = zh_entry\n",
        "            parts = [zh_entry]\n",
        "\n",
        "        parts.extend(synonyms)\n",
        "\n",
        "        description = \"ã€\".join(parts) + \"ã€‚é€™äº›æ˜¯ç›¸é—œèªç¾©æ“´å±•è³‡è¨Šã€‚\"\n",
        "        merged = f\"[Q] {standard_word} [SEP] {description}\"\n",
        "\n",
        "        texts.append(merged)\n",
        "        ids.append(f\"custom_{i}\")\n",
        "        metas.append({\n",
        "            \"term\": standard_word,\n",
        "            \"synonyms\": synonyms,\n",
        "            \"categories\": categories,\n",
        "            \"is_location\": False\n",
        "        })\n",
        "\n",
        "        for cat_name, city_list in categories.items():\n",
        "            for city in city_list:\n",
        "                texts.append(f\"[Q] {city} [SEP] {standard_word}åœ°å€\")\n",
        "                ids.append(f\"city_{i}_{city}\")\n",
        "                metas.append({\n",
        "                    \"term\": city,\n",
        "                    \"synonyms\": [],\n",
        "                    \"categories\": {},\n",
        "                    \"is_location\": True\n",
        "                })\n",
        "\n",
        "    vu.build_faiss_index_and_save(\n",
        "        texts=texts,\n",
        "        ids=ids,\n",
        "        meta_list=metas,\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        device=device,\n",
        "        index_path=index_path,\n",
        "        meta_path=meta_path,\n",
        "        pooling=pooling\n",
        "    )\n",
        "    print(f\"è‡ªè£½ Synonym å‘é‡åº«å·²å»ºç½®å®Œæˆï¼Œå…± {len(texts)} ç­†è³‡æ–™ã€‚\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_yI9cgtz-Cg"
      },
      "outputs": [],
      "source": [
        "# å»ºç«‹ NLPCC-MH èˆ‡è‡ªè£½ Synonyms å‘é‡åº«\n",
        "def run_all_indexing(nlpcc_input_path,nlpcc_index_path,nlpcc_meta_path,synonym_path,custom_index_path,custom_meta_path,model,tokenizer,device):\n",
        "    print(\"é–‹å§‹å…¨æµç¨‹å‘é‡åº«å»ºç«‹...\")\n",
        "\n",
        "    # å…ˆå»ºç«‹ NLPCC\n",
        "    print(\"\\né–‹å§‹å»ºç«‹ NLPCC-MH å‘é‡åº«...\")\n",
        "    prepare_nlpccmh_augmented_data(\n",
        "        input_path=nlpcc_input_path,\n",
        "        index_path=nlpcc_index_path,\n",
        "        meta_path=nlpcc_meta_path,\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        device=device)\n",
        "    # å»ºç«‹ Custom\n",
        "    print(\"\\né–‹å§‹å»ºç«‹è‡ªè£½ Synonym å‘é‡åº«...\")\n",
        "    prepare_custom_augmented_data(\n",
        "        synonym_path=synonym_path,\n",
        "        index_path=custom_index_path,\n",
        "        meta_path=custom_meta_path,\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        device=device)\n",
        "    print(\"\\nå…¨éƒ¨å‘é‡åº«å»ºç«‹å®Œæˆï¼\")\n",
        "# åŸ·è¡Œå»ºç«‹å‘é‡è³‡æ–™åº«\n",
        "run_all_indexing(nlpcc_input_path=\"/content/NLPCC-MH/data/nlpcc-mh.train_sememe.jsonl\",nlpcc_index_path=\"/content/index.faiss\",nlpcc_meta_path=\"/content/metadata.jsonl\",synonym_path=\"/content/sememe_synonym.json\",custom_index_path=\"/content/custom_index.faiss\",custom_meta_path=\"/content/custom_metadata.jsonl\",model=hf_model,tokenizer=tokenizer,device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**æŸ¥è©¢æ¸¬è©¦**"
      ],
      "metadata": {
        "id": "eCfwXaX9dUUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# è¼‰å…¥æ‰€æœ‰ç´¢å¼•\n",
        "index_paths = [(\"/content/index.faiss\", \"/content/metadata.jsonl\"),(\"/content/custom_index.faiss\", \"/content/custom_metadata.jsonl\")]\n",
        "\n",
        "indices, metadatas = [], []\n",
        "for idx_path, meta_path in index_paths:\n",
        "    index, meta = vu.load_index_and_metadata(idx_path, meta_path)\n",
        "    indices.append(index)\n",
        "    metadatas.append(meta)\n",
        "\n",
        "print(f\"æˆåŠŸè¼‰å…¥ {len(indices)} å€‹ç´¢å¼•åº«ï¼\")\n",
        "\n",
        "# æŸ¥è©¢\n",
        "query = \"[Q] æŸ³æ±Ÿçš„ä¸»è¦æ”¯æµçš„å‘æºåœ°æ˜¯å“ªé‡Œï¼Ÿ [SEP] ç™¼æºåœ° å«æœ‰èªæ„ã€Œplace|åœ°æ–¹ã€ComeToWorld|å•ä¸–ã€ï¼›æŸ³æ±Ÿ å«æœ‰èªæ„ã€Œwaters|æ°´åŸŸã€China|ä¸­åœ‹ã€\"\n",
        "\n",
        "# å–®ä¸€åº«æŸ¥è©¢\n",
        "single_results = vu.search_with_metadata(\n",
        "    query=query,\n",
        "    index=indices[0],\n",
        "    metadata=metadatas[0],\n",
        "    model=hf_model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=device,\n",
        "    topk=3\n",
        ")\n",
        "\n",
        "print(\"\\nå–®ä¸€åº«æŸ¥è©¢çµæœï¼š\")\n",
        "for r in single_results:\n",
        "    print(f\"åŒ¹é… ID: {r['id']}ï½œåˆ†æ•¸: {r['score']:.4f}\")\n",
        "    print(f\"åŸå§‹å•å¥/è©: {r['meta'].get('query', r['meta'].get('term', '')).strip()}\")\n",
        "    print(f\"èªç¾©æè¿°: {r['meta'].get('sememe', r['meta'].get('synonyms', ''))}\\n\")\n",
        "\n",
        "# å¤šåº«æ•´åˆæŸ¥è©¢\n",
        "combined_results = vu.combine_search(\n",
        "    query=query,\n",
        "    indices=indices,\n",
        "    metadatas=metadatas,\n",
        "    model=hf_model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=device,\n",
        "    topk=5\n",
        ")\n",
        "\n",
        "print(\"å¤šåº«æ•´åˆæŸ¥è©¢çµæœï¼š\")\n",
        "for r in combined_results:\n",
        "    print(f\"ä¾†è‡ªï¼š{r['source']}ï½œåŒ¹é… ID: {r['id']}ï½œåˆ†æ•¸: {r['score']:.4f}\")\n",
        "    print(f\"åŸå§‹å•å¥/è©: {r['meta'].get('query', r['meta'].get('term', '')).strip()}\")\n",
        "    print(f\"èªç¾©æè¿°: {r['meta'].get('sememe', r['meta'].get('synonyms', ''))}\\n\")"
      ],
      "metadata": {
        "id": "sxrBphuSdHAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0G3uR2Z0-lHP"
      },
      "outputs": [],
      "source": [
        "# å°‡æª¢ç´¢çµæœæ•´ç†ç‚º RAG è¼¸å…¥æ ¼å¼ã€‚\n",
        "def build_rag_prompt(query, retrieved_results, mode=\"standard\"):\n",
        "    context_parts = []\n",
        "    for r in retrieved_results:\n",
        "        if mode == \"standard\":\n",
        "            context_parts.append(\n",
        "                f\"- å•å¥ï¼š{r['meta'].get('query', r['text'])}\\n  èªç¾©ï¼š{r['meta'].get('sememe', '')}\"\n",
        "            )\n",
        "        elif mode == \"flat\":\n",
        "            context_parts.append(r['text'])\n",
        "\n",
        "    context_block = \"\\n\\n\".join(context_parts)\n",
        "    prompt = f\"ä½ æ˜¯ä¸€å€‹ä¸­æ–‡å•ç­”åŠ©ç†ï¼Œè«‹æ ¹æ“šä¸‹åˆ—æª¢ç´¢åˆ°çš„èªæ„è³‡è¨Šï¼Œå›ç­”åŸå§‹å•é¡Œï¼š\\n\\n{context_block}\\n\\nğŸ”¸ åŸå§‹å•é¡Œï¼š{query}\\n\\nâœ… è«‹ç”¨è‡ªç„¶èªè¨€å›ç­”ï¼š\"\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uf9Ms5W-PSH4"
      },
      "outputs": [],
      "source": [
        "# å–å¾— API é‡‘é‘°èˆ‡ headers\n",
        "api_key = userdata.get(\"Groq\")\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {api_key}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "# å–å¾—æ¨¡å‹åˆ—è¡¨\n",
        "r = requests.get(\"https://api.groq.com/openai/v1/models\", headers=headers)\n",
        "data = r.json()\n",
        "groq_model_id = data.get(\"data\", [])\n",
        "df = pd.DataFrame(groq_model_id)\n",
        "if \"id\" in df.columns:\n",
        "    df = df[[\"id\"]]\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    df.index += 1\n",
        "    display(df)\n",
        "\n",
        "# æŒ‡å®šä½¿ç”¨æ¨¡å‹\n",
        "groq_model_id =\"meta-llama/llama-4-maverick-17b-128e-instruct\"\n",
        "base_url = \"https://api.groq.com/openai/v1\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GceIlZrKQMuI"
      },
      "outputs": [],
      "source": [
        "# è¼‰å…¥ç´¢å¼•\n",
        "index1, meta1 = vu.load_index_and_metadata(index_path=\"/content/index.faiss\",meta_path=\"/content/metadata.jsonl\")\n",
        "index2, meta2 = vu.load_index_and_metadata(index_path=\"/content/custom_index.faiss\",meta_path=\"/content/custom_metadata.jsonl\")\n",
        "\n",
        "# æŸ¥è©¢å•é¡Œ\n",
        "query = \"æŸ³æ±Ÿçš„ä¸»è¦æ”¯æµçš„ç™¼æºåœ°æ˜¯ï¼Ÿ\"\n",
        "\n",
        "# ç”¨å‘é‡åº«æª¢ç´¢ï¼ˆå–®è¼ªæŸ¥è©¢ï¼‰\n",
        "search_results = vu.search_with_metadata(\n",
        "    query=query,\n",
        "    index=index1,\n",
        "    metadata=meta1,\n",
        "    model=hf_model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=device,\n",
        "    topk=3\n",
        ")\n",
        "\n",
        "print(\"å–®è¼ªæª¢ç´¢çµæœï¼š\")\n",
        "for r in search_results:\n",
        "    print(f\"- åŒ¹é…åˆ†æ•¸: {r['score']:.4f} ï½œå…§å®¹: {r['text'][:50]}...\")\n",
        "\n",
        "# å¤šå›åˆ QAï¼ˆä½¿ç”¨å…©å€‹ç´¢å¼•ï¼‰\n",
        "history = multi_turn_qa(initial_query=query,index_list=[(index1, meta1), (index2, meta2)],model=hf_model,tokenizer=tokenizer,device=device,api_key=api_key,max_turns=3,topk=5)\n",
        "\n",
        "# é¡¯ç¤ºæ•´å€‹å°è©±ç´€éŒ„\n",
        "print(\"\\nå¤šå›åˆå•ç­”ç´€éŒ„ï¼š\")\n",
        "for record in history:\n",
        "    print(f\"\\nå•é¡Œï¼š{record['query']}\")\n",
        "    print(f\"å›ç­”ï¼š{record['answer']}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# è¼‰å…¥ API é‡‘é‘°\n",
        "api_keyW = userdata.get(\"å¤©æ°£\")\n",
        "\n",
        "# ç¹ç°¡è½‰æ›\n",
        "cc_sp2tw = OpenCC('s2twp')\n",
        "\n",
        "# æ–‡å­—æ­£è¦åŒ–\n",
        "def normalize_text(text):\n",
        "    return cc_sp2tw.convert(text).replace(\"å°\", \"è‡º\").lower().strip()\n",
        "\n",
        "# å•å¥é è™•ç†\n",
        "def preprocess_query(query):\n",
        "    query = normalize_text(query)\n",
        "    useless_words = [\"è«‹å•\", \"ä»Šå¤©\", \"ç¾åœ¨\", \"å‘¢\", \"çš„\", \"è«‹\", \"ä¸€ä¸‹\", \"ç›®å‰\", \"å—\", \"å‘¢ï¼Ÿ\", \"ï¼Ÿ\"]\n",
        "    for word in useless_words:\n",
        "        query = query.replace(word, \"\")\n",
        "    return query"
      ],
      "metadata": {
        "id": "Sdcqhry78Obu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pq2Dwa8ZWXAl"
      },
      "outputs": [],
      "source": [
        "# ç¹ç°¡è½‰æ› + æ¸…æ´—\n",
        "cc_sp2tw = OpenCC('s2twp')\n",
        "\n",
        "def normalize_text(text):\n",
        "    return cc_sp2tw.convert(text).replace(\"å°\", \"è‡º\").lower().strip()\n",
        "\n",
        "def preprocess_query(query):\n",
        "    query = cc_sp2tw.convert(query)   # ç¹ç°¡è½‰æ›\n",
        "    query = query.replace(\"å°\", \"è‡º\").lower().strip()\n",
        "\n",
        "    noise_words = [\"è«‹å•\", \"ä¸€ä¸‹\", \"å‘¢\", \"å—\", \"ï¼Ÿ\", \"å‘¢ï¼Ÿ\", \"å‘¢å—\", \"è«‹\"]\n",
        "    for word in noise_words:\n",
        "        query = query.replace(word, \"\")\n",
        "    return query"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# æ–‡æœ¬å‘é‡åŒ–\n",
        "def encode_texts(texts, model, tokenizer, device, pooling=\"cls\", normalize_vec=True, batch_size=64):\n",
        "    if isinstance(texts, str):\n",
        "        texts = [texts]\n",
        "\n",
        "    all_embeddings = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        encodings = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\", max_length=64).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**encodings)\n",
        "        if pooling == \"cls\":\n",
        "            embeddings = outputs.last_hidden_state[:, 0]\n",
        "        else:\n",
        "            mask = encodings[\"attention_mask\"].unsqueeze(-1).expand(outputs.last_hidden_state.size()).float()\n",
        "            embeddings = (outputs.last_hidden_state * mask).sum(1) / mask.sum(1).clamp(min=1e-9)\n",
        "        all_embeddings.append(embeddings.cpu())\n",
        "\n",
        "    all_embeddings = torch.cat(all_embeddings, dim=0).numpy()\n",
        "    return normalize(all_embeddings) if normalize_vec else all_embeddings"
      ],
      "metadata": {
        "id": "eTcCih0REsgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HcmW6u1hOlq"
      },
      "outputs": [],
      "source": [
        "# æŸ¥è©¢å‘é‡åº«\n",
        "def query_custom_sememe(query, index, metas, model, tokenizer, device, topk=10, pooling=\"cls\", min_score=0.3):\n",
        "    clean_query = preprocess_query(query)\n",
        "    query_vec = encode_texts(clean_query, model, tokenizer, device, pooling)\n",
        "\n",
        "    D, I = index.search(query_vec, topk)\n",
        "\n",
        "    results = []\n",
        "    for score, idx in zip(D[0], I[0]):\n",
        "        if idx >= 0 and score >= min_score:\n",
        "            meta = metas[idx]\n",
        "            text = meta.get(\"term\") or meta.get(\"query\") or \"\"  # âš¡ é‡é»ï¼å®‰å…¨ fallback\n",
        "            results.append({\n",
        "                \"score\": float(score),\n",
        "                \"text\": text,\n",
        "                \"sememe\": meta.get(\"sememe\", \"\"),\n",
        "                \"is_location\": meta.get(\"is_location\", False)\n",
        "            })\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# åœ°åç°¡åŒ–æ˜ å°„è¡¨\n",
        "def build_city_alias_map(metas):\n",
        "    alias_map = {}\n",
        "    for meta in metas:\n",
        "        if meta.get(\"is_location\"):\n",
        "            original = meta.get(\"term\", \"\")\n",
        "            if original:\n",
        "                alias_map[original.replace(\"å°\", \"è‡º\")] = original\n",
        "                short = original.replace(\"è‡º\", \"å°\").replace(\"å¸‚\", \"\").replace(\"ç¸£\", \"\")\n",
        "                alias_map[short] = original\n",
        "    return alias_map\n",
        "\n",
        "# term-sememeå°æ‡‰è¡¨\n",
        "def build_term_sememe_map(metas):\n",
        "    term_map = {}\n",
        "    for meta in metas:\n",
        "        term = meta.get(\"term\", \"\")\n",
        "        sememe = meta.get(\"sememe\", \"\")\n",
        "        if term and sememe:\n",
        "            term_map[term] = sememe\n",
        "    return term_map\n",
        "\n",
        "# åœ°åfallbackä¿®æ­£\n",
        "def fix_location_name(name, city_alias_map):\n",
        "    for key, val in city_alias_map.items():\n",
        "        if key in name:\n",
        "            return val\n",
        "    return name"
      ],
      "metadata": {
        "id": "JqXUkZU78eRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# å¤©æ°£APIæŸ¥è©¢\n",
        "def fetch_weather(location, api_keyW):\n",
        "    try:\n",
        "        url = f\"http://api.openweathermap.org/data/2.5/weather?q={location}&appid={api_keyW}&units=metric&lang=zh_tw\"\n",
        "        response = requests.get(url, timeout=5)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        return {\n",
        "            \"condition\": data[\"weather\"][0][\"description\"],\n",
        "            \"temperature\": data[\"main\"][\"temp\"],\n",
        "            \"humidity\": data[\"main\"][\"humidity\"],\n",
        "            \"pressure\": data[\"main\"][\"pressure\"],\n",
        "            \"wind_speed\": data[\"wind\"][\"speed\"]\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"å¤©æ°£APIéŒ¯èª¤: {e} | ä½ç½®ï¼š{location}\")\n",
        "        return None\n",
        "\n",
        "# æ ¼å¼åŒ–å›è¦†\n",
        "def format_weather_reply(weather_data, location):\n",
        "    if not weather_data:\n",
        "        return f\"ç„¡æ³•å–å¾— {location} çš„å¤©æ°£è³‡æ–™ã€‚\"\n",
        "    return (\n",
        "        f\"{location} å¤©æ°£æ¦‚æ³ï¼š\\n\"\n",
        "        f\"ç‹€æ…‹ï¼š{weather_data['condition']}\\n\"\n",
        "        f\"æ°£æº«ï¼š{weather_data['temperature']}Â°C\\n\"\n",
        "        f\"æ¿•åº¦ï¼š{weather_data['humidity']}%\\n\"\n",
        "        f\"é¢¨é€Ÿï¼š{weather_data['wind_speed']} m/s\\n\"\n",
        "        f\"æ°£å£“ï¼š{weather_data['pressure']} hPa\"\n",
        "    )"
      ],
      "metadata": {
        "id": "DcUDijm-8iM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# å‘é‡æª¢ç´¢ + åœ°åä¿®æ­£ + fallbackå›è¦†\n",
        "def handle_weather_query(user_query, index, metas, model, tokenizer, device, city_alias_map, term_sememe_map, api_keyW, topk=10, min_score=0.3):\n",
        "    results = query_custom_sememe(user_query, index, metas, model, tokenizer, device, topk=topk, min_score=min_score)\n",
        "\n",
        "    found_locations = []\n",
        "    found_weather = False\n",
        "\n",
        "    vague_terms = {\"åŸå¸‚\", \"å¸‚å€\", \"éƒ½å¸‚\", \"éƒ½æœƒå€\", \"å¸‚é®\"}\n",
        "\n",
        "    for r in results:\n",
        "        if r[\"is_location\"] and r[\"text\"] not in vague_terms:\n",
        "            fixed_location = fix_location_name(r[\"text\"], city_alias_map)\n",
        "            found_locations.append(fixed_location)\n",
        "        elif not r[\"is_location\"] and (\"å¤©æ°£\" in r[\"sememe\"] or \"æ°£å€™\" in r[\"sememe\"]):\n",
        "            found_weather = True\n",
        "\n",
        "    found_locations = list(set(found_locations))\n",
        "\n",
        "    if found_locations and found_weather:\n",
        "        replies = []\n",
        "        for loc in found_locations:\n",
        "            weather_data = fetch_weather(loc, api_keyW)\n",
        "            reply = format_weather_reply(weather_data, loc)\n",
        "            replies.append(reply)\n",
        "        return \"\\n\\n\".join(replies)\n",
        "\n",
        "    elif found_weather and not found_locations:\n",
        "        return \"è«‹å•æ‚¨æƒ³æŸ¥è©¢å“ªå€‹åŸå¸‚æˆ–åœ°å€çš„å¤©æ°£å‘¢ï¼Ÿ\"\n",
        "\n",
        "    elif found_locations and not found_weather:\n",
        "        replies = []\n",
        "        for loc in found_locations:\n",
        "            weather_data = fetch_weather(loc, api_keyW)\n",
        "            reply = format_weather_reply(weather_data, loc)\n",
        "            replies.append(reply)\n",
        "        return \"\\n\\n\".join(replies)\n",
        "\n",
        "    else:\n",
        "        return \"ç›®å‰ç„¡æ³•è¾¨è­˜æ‚¨çš„å•é¡Œæ˜¯å¦èˆ‡å¤©æ°£æœ‰é—œï¼Œè«‹é‡æ–°æè¿°åœ°é»èˆ‡å…§å®¹ã€‚\""
      ],
      "metadata": {
        "id": "Z3O47fvF8ukc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# è¼‰å…¥ custom index\n",
        "index_custom = faiss.read_index(\"/content/custom_index.faiss\")\n",
        "with open(\"/content/custom_metadata.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    meta_custom = [json.loads(line) for line in f]\n",
        "\n",
        "# è£½ä½œ alias_map å’Œ sememe_map\n",
        "city_alias_map = build_city_alias_map(meta_custom)\n",
        "term_sememe_map = build_term_sememe_map(meta_custom)\n",
        "\n",
        "# åˆå§‹åŒ– tokenizer + model\n",
        "model_name = \"BAAI/bge-base-zh\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = next(model.parameters()).device"
      ],
      "metadata": {
        "id": "x-VsUYzfHJqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# æ¸¬è©¦ç°¡ç‰ˆ\n",
        "query = \"ä»Šå¤©å°ä¸­æœƒä¸‹é›¨å—ï¼Ÿ\"\n",
        "print(preprocess_query(query))  # --> ç¢ºèªæœ‰è‡ºä¸­å’Œä¸‹é›¨å­—çœ¼\n",
        "results = query_custom_sememe(query, index_custom, meta_custom, model, tokenizer, device, topk=10, min_score=0.2)\n",
        "\n",
        "for r in results:\n",
        "    print(r)"
      ],
      "metadata": {
        "id": "aw_4_XxNNYV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# æ¸¬è©¦\n",
        "user_query = \"ä»Šå¤©å°ä¸­æœƒä¸‹é›¨å—ï¼Ÿ\"\n",
        "final_answer = handle_weather_query(user_query, index_custom, meta_custom, model, tokenizer, device, city_alias_map, term_sememe_map, api_keyW)\n",
        "print(final_answer)"
      ],
      "metadata": {
        "id": "waMRhV5iHMh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# èª¿ç”¨ Groq APIï¼Œæ ¹æ“šæª¢ç´¢çµæœç”Ÿæˆå¼·åŒ–å›ç­”\n",
        "def generate_answer_with_groq(user_input, results, api_key, model=\"mixtral-8x7b-32768\", temperature=0.2):\n",
        "    context_block = \"\"\n",
        "    for idx, r in enumerate(results):\n",
        "        query = r['meta'].get('query', '')\n",
        "        sememe = r['meta'].get('sememe', '')\n",
        "        context_block += f\"[{idx+1}] {query} | {sememe}\\n\"\n",
        "\n",
        "    prompt = (\n",
        "        f\"è«‹æ ¹æ“šä»¥ä¸‹å·²çŸ¥è³‡è¨Šï¼Œå›ç­”ç”¨æˆ¶çš„å•é¡Œã€‚\\n\\n\"\n",
        "        f\"===å·²çŸ¥çŸ¥è­˜===\\n{context_block}\\n\\n\"\n",
        "        f\"===ç”¨æˆ¶æå•===\\n{user_input}\\n\\n\"\n",
        "        f\"è«‹ä¿æŒç°¡æ½”ã€æ¸…æ¥šï¼Œä¸¦é¿å…ç·¨é€ ã€‚\"\n",
        "    )\n",
        "\n",
        "    url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    payload = {\n",
        "        \"model\": model,\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ä¸­æ–‡å•ç­”åŠ©æ‰‹ã€‚\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        \"temperature\": temperature,\n",
        "        \"top_p\": 1.0,\n",
        "        \"max_tokens\": 1024,\n",
        "        \"n\": 1\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(url, headers=headers, json=payload, timeout=15)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        return data[\"choices\"][0][\"message\"][\"content\"]\n",
        "    except Exception as e:\n",
        "        print(f\"Groq APIå¤±æ•—: {e}\")\n",
        "        return \"ç›®å‰ç„¡æ³•å–å¾—å›ç­”ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚\""
      ],
      "metadata": {
        "id": "_lYGol1su4FL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def master_pipeline(user_input):\n",
        "    if is_weather_query(user_input):\n",
        "        return handle_weather_query(\n",
        "            user_query=user_input,\n",
        "            index=index_custom,\n",
        "            metas=meta_custom,\n",
        "            model=model_vector,\n",
        "            tokenizer=tokenizer_vector,\n",
        "            device=device_vector,\n",
        "            city_alias_map=city_alias_map,\n",
        "            term_sememe_map=term_sememe_map,\n",
        "            api_keyW=api_keyW,\n",
        "            topk=10,\n",
        "            min_score=0.3\n",
        "        )\n",
        "    else:\n",
        "        results = combine_search(\n",
        "            query=user_input,\n",
        "            indices=[index_nlpcc, index_custom],\n",
        "            metadatas=[meta_nlpcc, meta_custom],\n",
        "            model_vector=model_vector,\n",
        "            tokenizer_vector=tokenizer_vector,\n",
        "            device_vector=device_vector,\n",
        "            topk=5\n",
        "        )\n",
        "        return generate_answer_with_groq(user_input, results, api_key, model=groq_model_id)"
      ],
      "metadata": {
        "id": "3boadZuouZa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# è¼‰å…¥å‘é‡åº«èˆ‡metadata\n",
        "index_nlpcc, meta_nlpcc = vu.load_index_and_metadata(\"/content/index.faiss\", \"/content/metadata.jsonl\")\n",
        "index_custom, meta_custom = vu.load_index_and_metadata(\"/content/custom_index.faiss\", \"/content/custom_metadata.jsonl\")\n",
        "\n",
        "# è¼‰å…¥å¤©æ°£å‘é‡åº«\n",
        "index_weather, meta_weather = vu.load_index_and_metadata(\"/content/weather_sememe_index.faiss\", \"/content/weather_sememe_metadata.jsonl\")\n",
        "\n",
        "# è¼‰å…¥çŸ¥è­˜åº«ç”¨å‘é‡åŒ–æ¨¡å‹\n",
        "model_vector_name = \"BAAI/bge-base-zh\"\n",
        "tokenizer_vector = AutoTokenizer.from_pretrained(model_vector_name)\n",
        "model_vector = AutoModel.from_pretrained(model_vector_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device_vector = next(model_vector.parameters()).device"
      ],
      "metadata": {
        "id": "60lMM16NuV2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcuxTDSOg_Fi"
      },
      "outputs": [],
      "source": [
        "# æ¸¬è©¦\n",
        "user_question = \"ä»Šå¤©å°ä¸­æœƒä¸‹é›¨å—ï¼Ÿ\"\n",
        "final_answer = master_pipeline(user_question)\n",
        "print(final_answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaOBErHbTDCU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "624dbc67-4e83-4ba7-bbeb-5037cff0ce06"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "Failed to download file. Status code: 400",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-f0622ea36bd6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"https://raw.githubusercontent.com/your_repo/your_logo.png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Logoï¼ˆå¯ä»¥æ›ç¶²å€æˆ–æ”¾æœ¬åœ°ï¼‰\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mchatbot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChatbot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"å¤©æ°£å‹ AI æ™ºèƒ½åŠ©ç†\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio/component_meta.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio/components/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, value, format, height, width, image_mode, sources, type, label, every, inputs, show_label, show_download_button, container, scale, min_width, interactive, visible, streaming, elem_id, elem_classes, render, key, mirror_webcam, webcam_options, show_share_button, placeholder, show_fullscreen_button, webcam_constraints)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplaceholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mevery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio/components/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mStreamingInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetaclass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mABCMeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio/component_meta.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio/components/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, value, label, info, show_label, container, scale, min_width, interactive, visible, elem_id, elem_classes, render, key, load_fn, every, inputs)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0minitial_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitial_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         self.value = move_files_to_cache(\n\u001b[0m\u001b[1;32m    228\u001b[0m             \u001b[0minitial_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio/processing_utils.py\u001b[0m in \u001b[0;36mmove_files_to_cache\u001b[0;34m(data, block, postprocess, check_in_upload_folder, keep_in_cache)\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m     return client_utils.traverse(\n\u001b[0m\u001b[1;32m    528\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_move_to_cache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_file_obj_with_meta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio_client/utils.py\u001b[0m in \u001b[0;36mtraverse\u001b[0;34m(json_obj, func, is_root)\u001b[0m\n\u001b[1;32m   1103\u001b[0m     \"\"\"\n\u001b[1;32m   1104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_root\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0mnew_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio/processing_utils.py\u001b[0m in \u001b[0;36m_move_to_cache\u001b[0;34m(d)\u001b[0m\n\u001b[1;32m    499\u001b[0m                 \u001b[0m_check_allowed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_in_upload_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpayload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m                 \u001b[0mtemp_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove_resource_to_block_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtemp_file_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Did not determine a file path for the resource.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\u001b[0m in \u001b[0;36mmove_resource_to_block_cache\u001b[0;34m(self, url_or_file_path)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclient_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_http_url_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m             temp_file_path = processing_utils.save_url_to_cache(\n\u001b[0m\u001b[1;32m    345\u001b[0m                 \u001b[0murl_or_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGRADIO_CACHE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio/processing_utils.py\u001b[0m in \u001b[0;36mssrf_protected_download\u001b[0;34m(url, cache_dir)\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munsafe_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m         return client_utils.synchronize_async(\n\u001b[0m\u001b[1;32m    399\u001b[0m             \u001b[0masync_ssrf_protected_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio_client/utils.py\u001b[0m in \u001b[0;36msynchronize_async\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m     \"\"\"\n\u001b[0;32m--> 890\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfsspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masyn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfsspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masyn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/fsspec/asyn.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(loop, func, timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFSTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mreturn_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mreturn_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreturn_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/fsspec/asyn.py\u001b[0m in \u001b[0;36m_runner\u001b[0;34m(event, coro, result, timeout)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mcoro\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoro\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio/processing_utils.py\u001b[0m in \u001b[0;36masync_ssrf_protected_download\u001b[0;34m(url, cache_dir)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Failed to download file. Status code: {response.status_code}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0maiofiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_temp_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: Failed to download file. Status code: 400"
          ]
        }
      ],
      "source": [
        "# Chat ä¸»é‚è¼¯\n",
        "def chat_with_agent(user_input, history):\n",
        "    history = history or []\n",
        "    reply = master_pipeline(user_input)\n",
        "    history.append((user_input, reply))\n",
        "    return history, history\n",
        "\n",
        "# å•Ÿå‹• Gradio\n",
        "with gr.Blocks(theme=gr.themes.Base(primary_hue=\"cyan\")) as demo:\n",
        "    gr.Markdown(\"\"\"\n",
        "    # ä¸­æ–‡èªç¾©å•ç­”åŠ©ç† + å¤©æ°£å°å¹«æ‰‹\n",
        "     æ”¯æ´å³æ™‚å¤©æ°£æŸ¥è©¢\n",
        "     æ”¯æ´èªç¾©æ“´å±•æ¨ç†\n",
        "     RAG + LLM æ™ºèƒ½ç”Ÿæˆ\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Image(value=\"https://raw.githubusercontent.com/your_repo/your_logo.png\", height=100)  # Logoï¼ˆå¯ä»¥æ›ç¶²å€æˆ–æ”¾æœ¬åœ°ï¼‰\n",
        "        with gr.Column(scale=5):\n",
        "            chatbot = gr.Chatbot(height=500, label=\"å¤©æ°£å‹ AI æ™ºèƒ½åŠ©ç†\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=4):\n",
        "            user_input = gr.Textbox(\n",
        "                placeholder=\"è«‹è¼¸å…¥å•é¡Œï¼Œä¾‹å¦‚ï¼šæ˜å¤©é«˜é›„æœƒä¸‹é›¨å—ï¼Ÿ\",\n",
        "                show_label=False\n",
        "            )\n",
        "        with gr.Column(scale=1, min_width=80):\n",
        "            submit_btn = gr.Button(\"é€å‡º\", variant=\"primary\")\n",
        "\n",
        "    clear_btn = gr.Button(\"ğŸ§¹ æ¸…é™¤å°è©±\", variant=\"secondary\")\n",
        "\n",
        "    # ç¶å®šäº¤äº’\n",
        "    history_state = gr.State([])\n",
        "\n",
        "    submit_btn.click(\n",
        "        chat_with_agent,\n",
        "        inputs=[user_input, history_state],\n",
        "        outputs=[chatbot, history_state]\n",
        "    )\n",
        "    user_input.submit(\n",
        "        chat_with_agent,\n",
        "        inputs=[user_input, history_state],\n",
        "        outputs=[chatbot, history_state]\n",
        "    )\n",
        "    clear_btn.click(lambda: ([], []), inputs=None, outputs=[chatbot, history_state])\n",
        "\n",
        "# å•Ÿå‹•æœå‹™\n",
        "demo.launch(share=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}