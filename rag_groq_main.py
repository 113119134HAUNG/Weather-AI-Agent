import os
import torch
import requests
from userdata import get
import vector_utils_advanced as vu
from transformers import AutoTokenizer, AutoModel
# ========== Groq API è¨­å®š ==========
api_key = get("Groq")
headers = {
    "Authorization": f"Bearer {api_key}",
    "Content-Type": "application/json"
}
groq_model_id = "meta-llama/llama-4-maverick-17b-128e-instruct"
api_url = "https://api.groq.com/openai/v1/chat/completions"

# ========== è¼‰å…¥ç´¢å¼• ==========
index_paths = [
    ("/content/index.faiss", "/content/metadata.jsonl"),
    ("/content/custom_index.faiss", "/content/custom_metadata.jsonl")
]
indices, metadatas = [], []
for idx_path, meta_path in index_paths:
    index, meta = vu.load_index_and_metadata(idx_path, meta_path)
    indices.append(index)
    metadatas.append(meta)
print(f"æˆåŠŸè¼‰å…¥ {len(indices)} å€‹ç´¢å¼•åº«ï¼")

# ========== å»ºç«‹ RAG Prompt ==========
def build_rag_prompt(query, retrieved_results, history=[], mode="standard"):
    context_parts = []
    for r in retrieved_results:
        if mode == "standard":
            context_parts.append(
                f"- å•å¥ï¼š{r['meta'].get('query', r['text'])}\n  èªç¾©ï¼š{r['meta'].get('sememe', '')}"
            )
        else:
            context_parts.append(r.get("text", ""))
    context_block = "\n\n".join(context_parts)

    history_block = "\n".join([f"ä½¿ç”¨è€…ï¼š{q}\nåŠ©ç†ï¼š{a}" for q, a in history])

    prompt = f"""ä½ æ˜¯ä¸€ä½ä¸­æ–‡èªæ„å•ç­”åŠ©ç†ï¼Œæ“…é•·æ•´åˆèªæ„è³‡è¨Šèˆ‡æª¢ç´¢ç‰‡æ®µã€‚è«‹æ ¹æ“šä¸‹åˆ—èªæ„è³‡æ–™èˆ‡ä¸Šä¸‹æ–‡ç´€éŒ„ï¼Œå›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚

ã€èªæ„æª¢ç´¢çµæœã€‘
{context_block}

ã€å°è©±ç´€éŒ„ã€‘
{history_block}

ğŸ”¸ ä½¿ç”¨è€…æå•ï¼š{query}

è«‹æ ¹æ“šèªæ„å…§å®¹ï¼Œä»¥è‡ªç„¶èªè¨€ç°¡æ½”å›ç­”ï¼š"""
    return prompt

# ========== æª¢ç´¢ä¸¦é¡¯ç¤º ==========
def run_query_and_get_results(query, indices, metadatas, model, tokenizer, device, topk=5):
    results = vu.combine_search(
        query=query,
        indices=indices,
        metadatas=metadatas,
        model=model,
        tokenizer=tokenizer,
        device=device,
        topk=topk
    )
    print("\næª¢ç´¢çµæœæ‘˜è¦ï¼ˆTopKï¼‰ï¼š")
    for r in results:
        print(f"ä¾†è‡ªï¼š{r['source']}ï½œåˆ†æ•¸: {r['score']:.4f}")
        print(f"å•å¥/è©ï¼š{r['meta'].get('query', r['meta'].get('term', '')).strip()}")
        print(f"èªç¾©æè¿°ï¼š{r['meta'].get('sememe', r['meta'].get('synonyms', ''))}\n")
    return results

# ========== å‘¼å« Groq API ==========
def generate_answer_with_groq(prompt):
    payload = {
        "model": groq_model_id,
        "messages": [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸­æ–‡èªæ„å•ç­”å°ˆå®¶ï¼Œæ“…é•·èªæ„ç†è§£èˆ‡çŸ¥è­˜æ•´åˆã€‚"},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.3
    }
    try:
        response = requests.post(api_url, headers=headers, json=payload, timeout=60)
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"].strip()
    except Exception as e:
        return f"å‘¼å« Groq API å¤±æ•—ï¼š{str(e)}"

# ========== å•Ÿå‹•å¤šå›åˆå•ç­” ==========
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
tokenizer = AutoTokenizer.from_pretrained("BAAI/bge-base-zh")
hf_model = AutoModel.from_pretrained("BAAI/bge-base-zh").to(device).eval()

chat_history = []
print("\næ­¡è¿ä½¿ç”¨ Semantic RAG å¤šè¼ªå•ç­”ç³»çµ±")
print("è¼¸å…¥ `exit` çµæŸå°è©±ã€‚\n")

while True:
    user_input = input("ä½¿ç”¨è€…ï¼š")
    if user_input.strip().lower() in {"exit", "quit", "bye", "å†è¦‹", "è¬è¬"}:
        print("å°è©±çµæŸï¼Œè¬è¬ä½¿ç”¨ï¼")
        break

    search_results = run_query_and_get_results(user_input, indices, metadatas, hf_model, tokenizer, device)
    prompt = build_rag_prompt(user_input, search_results, history=chat_history, mode="standard")
    answer = generate_answer_with_groq(prompt)

    print("\nGroq å›ç­”ï¼š", answer)
    chat_history.append((user_input, answer))